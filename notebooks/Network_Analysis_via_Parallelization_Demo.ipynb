{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "riA1WwTTID3q"
   },
   "source": [
    "## NHD Network Analysis Demo\n",
    "___TL;DR___: *We are trying to parallelize hydraulic calculations for dynamic subsets of the U.S. river and stream network*<br><br>\n",
    "The following was developed as part of the process of preparing a method for forecasting flows on the US network of rivers and streams as represented in the National Hydrography Dataset (NHD). The NHD is a continuously evolving characterization of a fractal system so we felt that we needed to plan to have some flexibility. We hope to identify the complexity inherent in the network at different levels of resolution and we hope to be able to do so dynamically. The goal is also to be able to manage the complexity calculation for arbitrary collections of headwater points, such as might be obtained from a list of named streams or during a major flood event in a particular region.<br>\n",
    "As a point of terminology, we use the word 'routing' as shorthand to refer to the computation of the translation of a particular flow condition, high or low, to downstream (or in some cases upstream) areas of influence.\n",
    "The network complexity is related to the potential for parallelization of a serial analysis of the network. We have identified three levels of parallelization that may be implemented: \n",
    "1. System-level parallelization of independent networks -- the routing computations for the Mississippi River have little (nothing, except conceptual similarity and a shared existence on earth) to do with the computations for the Columbia river for any practical level of analysis. The system of networks across the US is what we are considering in general.\n",
    "1. Network-level parallelization of interconnected reaches -- There is a need to consider the computations for adjacent branches within a network of con-flowing streams, but with proper ordering, some of the computations may be considered in parallel. For example, the Illinois River headwaters and the Mississippi River headwaters are related within their broader Mississippi network, but the routing calculations for those headwaters are pratically agnostic to one another.\n",
    "1. Reach-level parallelization of the specific routing computation -- the numerical work of routing water downstream is a matrix computation and consists of exploring solutions to differential equations, all of which may potentially be examined in parallel, under the proper conditions and with suitable assumptions.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qv_SVt3VzC9J"
   },
   "source": [
    "### Import the git repo including test data\n",
    "This git repo is a fork/branch of the national water model public repository hosted by NCAR. The NCAR repo is the basis for the WRF-Hydro model that is presently the modeling engine of the [US National Water Model.](https://water.noaa.gov/about/nwm)<br>\n",
    "\n",
    "The network analysis code assumes that the downstream neighbor is identified in the table for each stream segment as is the case for the test datasets. \n",
    "\n",
    "We recognize that others have done similar work and may possibly have done it better. We are working on being more able to nimbly respond to suggestions and opportunities for improvement. Please let us know if you see something we could do better or of course feel free to fork and improve what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "0RBaWmyY7Z88",
    "outputId": "5e88d24d-9547-4347-eb77-509384696c2b"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    ENV_IS_CL = True\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"git\",\n",
    "            \"clone\",\n",
    "            \"--single-branch\",\n",
    "            \"--branch\",\n",
    "            \"master\",\n",
    "            \"https://github.com/NOAA-OWP/t-route.git\",\n",
    "        ]\n",
    "    )\n",
    "    sys.path.append(\"/content/src/python_framework_v01\")\n",
    "    subprocess.run([\"pip\", \"install\", \"geopandas\"])\n",
    "    subprocess.run([\"pip\", \"install\", \"netcdf4\"])\n",
    "    # default recursion limit (~1000) is slightly too small for the deepest branches of the network\n",
    "    sys.setrecursionlimit(6000)\n",
    "    # TODO: convert recursive functions to stack-based functions\n",
    "except:\n",
    "    ENV_IS_CL = False\n",
    "    sys.path.append(r\"../src/python_framework_v01\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mJ0UirjC0iN1"
   },
   "source": [
    "### Some general functions and a test case\n",
    "The next blocks use functions for several modules (`nhd_network_utilities` and `recursive_print`) from the git repository, to create the `connnections` objects which characterize the network to be analyzed.<br><br>\n",
    "The `test_rows` object simulates a river network dataset such as we recieve from the National Hydrography Dataset. Each data row has a node ID, a 'to' node ID, and some other relevant data. For this test dataset, the second data column is a dummy length (and the last column could be some other value, but we haven't tried anything yet... stay tuned) and in our traversals, we can add up the lengths as a surrogate for more complex water routing functions we need to eventually manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "KFnY-vZDYPue",
    "outputId": "58c2d71e-59d2-4d4e-818b-2dea1a337471"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Executing Test\n",
      "########################\n",
      "Downstream Connections\n",
      "########################\n",
      "1 with length 178\n",
      "4 with length 798\n",
      "0 with length 456\n",
      "########################\n",
      "3 with length 301\n",
      "2 with length 394\n",
      "0 with length 456\n",
      "########################\n",
      "5 with length 679\n",
      "4 with length 798\n",
      "0 with length 456\n",
      "########################\n",
      "6 with length 523\n",
      "0 with length 456\n",
      "########################\n",
      "7 with length 815\n",
      "2 with length 394\n",
      "0 with length 456\n",
      "########################\n",
      "20 with length 543\n",
      "19 with length 832\n",
      "18 with length 458\n",
      "17 with length 514\n",
      "16 with length 920\n",
      "15 with length 920\n",
      "14 with length 548\n",
      "13 with length 240\n",
      "12 with length 543\n",
      "11 with length 832\n",
      "10 with length 458\n",
      "9 with length 514\n",
      "8 with length 841\n",
      "########################\n",
      "24 with length 240\n",
      "23 with length 920\n",
      "22 with length 548\n",
      "21 with length 240\n",
      "16 with length 920\n",
      "15 with length 920\n",
      "14 with length 548\n",
      "13 with length 240\n",
      "12 with length 543\n",
      "11 with length 832\n",
      "10 with length 458\n",
      "9 with length 514\n",
      "8 with length 841\n",
      "########################\n",
      "28 with length 920\n",
      "27 with length 920\n",
      "26 with length 920\n",
      "25 with length 548\n",
      "12 with length 543\n",
      "11 with length 832\n",
      "10 with length 458\n",
      "9 with length 514\n",
      "8 with length 841\n",
      "########################\n",
      "########################\n",
      "Upstream Connections\n",
      "########################\n",
      "\\0 with length 456\\\n",
      "..\\2 with length 394\\\n",
      "....\\3 with length 301\\\n",
      "....\\7 with length 815\\\n",
      "..\\4 with length 798\\\n",
      "....\\1 with length 178\\\n",
      "....\\5 with length 679\\\n",
      "..\\6 with length 523\\\n",
      "########################\n",
      "\\8 with length 841\\\n",
      "..\\9 with length 514\\\n",
      "....\\10 with length 458\\\n",
      "......\\11 with length 832\\\n",
      "........\\12 with length 543\\\n",
      "..........\\25 with length 548\\\n",
      "............\\26 with length 920\\\n",
      "..............\\27 with length 920\\\n",
      "................\\28 with length 920\\\n",
      "..........\\13 with length 240\\\n",
      "............\\14 with length 548\\\n",
      "..............\\15 with length 920\\\n",
      "................\\16 with length 920\\\n",
      "..................\\17 with length 514\\\n",
      "....................\\18 with length 458\\\n",
      "......................\\19 with length 832\\\n",
      "........................\\20 with length 543\\\n",
      "..................\\21 with length 240\\\n",
      "....................\\22 with length 548\\\n",
      "......................\\23 with length 920\\\n",
      "........................\\24 with length 240\\\n",
      "########################\n",
      "Total Segments 43\n",
      "Head Segments 8\n",
      "found 6 junctions\n",
      "...in 5 junction nodes\n",
      "Total Reaches ( = head_segments + junction_nodes ) 13\n",
      "Total Independent Networks (estimated from number of terminal keys) 2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nhd_network_utilities_v01 as nnu\n",
    "import recursive_print\n",
    "\n",
    "# def main():\n",
    "if 1 == 1:\n",
    "    \"\"\"##TEST\"\"\"\n",
    "    print(\"\")\n",
    "    print(\"Executing Test\")\n",
    "    # Test data\n",
    "    test_rows = [\n",
    "        [50, 178, 51, 0],\n",
    "        [51, 178, 50, 0],\n",
    "        [60, 178, 61, 0],\n",
    "        [61, 178, 62, 0],\n",
    "        [62, 178, 60, 0],\n",
    "        [70, 178, 71, 0],\n",
    "        [71, 178, 72, 0],\n",
    "        [72, 178, 73, 0],\n",
    "        [73, 178, 70, 0],\n",
    "        [80, 178, 81, 0],\n",
    "        [81, 178, 82, 0],\n",
    "        [82, 178, 83, 0],\n",
    "        [83, 178, 84, 0],\n",
    "        [84, 178, 80, 0],\n",
    "        [0, 456, -999, 0],\n",
    "        [1, 178, 4, 0],\n",
    "        [2, 394, 0, 0],\n",
    "        [3, 301, 2, 0],\n",
    "        [4, 798, 0, 0],\n",
    "        [5, 679, 4, 0],\n",
    "        [6, 523, 0, 0],\n",
    "        [7, 815, 2, 0],\n",
    "        [8, 841, -999, 0],\n",
    "        [9, 514, 8, 0],\n",
    "        [10, 458, 9, 0],\n",
    "        [11, 832, 10, 0],\n",
    "        [12, 543, 11, 0],\n",
    "        [13, 240, 12, 0],\n",
    "        [14, 548, 13, 0],\n",
    "        [15, 920, 14, 0],\n",
    "        [16, 920, 15, 0],\n",
    "        [17, 514, 16, 0],\n",
    "        [18, 458, 17, 0],\n",
    "        [19, 832, 18, 0],\n",
    "        [20, 543, 19, 0],\n",
    "        [21, 240, 16, 0],\n",
    "        [22, 548, 21, 0],\n",
    "        [23, 920, 22, 0],\n",
    "        [24, 240, 23, 0],\n",
    "        [25, 548, 12, 0],\n",
    "        [26, 920, 25, 0],\n",
    "        [27, 920, 26, 0],\n",
    "        [28, 920, 27, 0],\n",
    "    ]\n",
    "\n",
    "    test_key_col = 0\n",
    "    test_downstream_col = 2\n",
    "    test_length_col = 1\n",
    "    test_terminal_code = -999\n",
    "    debuglevel = 0\n",
    "    verbose = False\n",
    "\n",
    "    (\n",
    "        connections,\n",
    "        all_keys,\n",
    "        ref_keys,\n",
    "        headwater_keys,\n",
    "        terminal_keys,\n",
    "        terminal_ref_keys,\n",
    "        circular_keys,\n",
    "        junction_keys,\n",
    "        visited_keys,\n",
    "        visited_terminal_keys,\n",
    "        junction_count,\n",
    "        confluence_segment_set,\n",
    "        waterbody_dict,\n",
    "        waterbody_segments,\n",
    "        waterbody_outlet_set,\n",
    "        waterbody_upstreams_set,\n",
    "        waterbody_downstream_set,\n",
    "    ) = nnu.build_connections_object(\n",
    "        geo_file_rows=test_rows,\n",
    "        key_col=test_key_col,\n",
    "        mask_set={row[test_key_col] for row in test_rows},\n",
    "        downstream_col=test_downstream_col,\n",
    "        length_col=test_length_col,\n",
    "        terminal_code=test_terminal_code,\n",
    "        verbose=verbose,\n",
    "        debuglevel=debuglevel,\n",
    "    )\n",
    "    # for index, value in enumerate(test_return_values):\n",
    "    #     print(f\"{index}. {str(value)[:100]} [{type(value).__name__}]\")\n",
    "    recursive_print.print_connections(\n",
    "        headwater_keys=headwater_keys,\n",
    "        down_connections=connections,\n",
    "        up_connections=connections,\n",
    "        terminal_code=test_terminal_code,\n",
    "        terminal_keys=terminal_keys,\n",
    "        terminal_ref_keys=terminal_ref_keys,\n",
    "        debuglevel=debuglevel,\n",
    "    )\n",
    "\n",
    "    recursive_print.print_basic_network_info(\n",
    "        connections=connections,\n",
    "        headwater_keys=headwater_keys,\n",
    "        junction_keys=junction_keys,\n",
    "        terminal_keys=terminal_keys,\n",
    "        terminal_code=test_terminal_code,\n",
    "        verbose=verbose,\n",
    "        debuglevel=debuglevel,\n",
    "    )\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5xhb-Q4pCbX"
   },
   "source": [
    "### Real Networks\n",
    "(you can skip this cell to test the code on the simple case generated above...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "il50SP8eZBk2",
    "outputId": "32c43141-2c53-4919-b1e6-a81dd9f6dc92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brazos_LowerColorado_ge5\n",
      "reading -- dataset: /home/chp20/github/t-route/test/input/geo/Channels/NHD_BrazosLowerColorado_Channels.shp; layer: 0; driver: ESRI Shapefile\n",
      "   OBJECTID_1  OBJECTID  featureID  linkDim     link  order_  Length       to  \\\n",
      "0        1499     25442    3764288   460086  3764288       6  2150.0  3764296   \n",
      "1        1500     25443    3764296   460088  3764296       6  1277.0  3765756   \n",
      "2        1501     25444    3766380   460092  3766380       6   431.0  3766382   \n",
      "3        1502     25445    3765756   460090  3765756       6  1274.0  3766380   \n",
      "4        1503     25447    3765796   460180  3765796       6   219.0  3765798   \n",
      "\n",
      "     MusK  MusX   So     n  BtmWdth  ChSlp   Qi gages  NHDWaterbo      lat  \\\n",
      "0  3600.0   0.2  0.0  0.05  39.8032   0.05  0.0  None       -9999  29.0176   \n",
      "1  3600.0   0.2  0.0  0.05  39.8543   0.05  0.0  None       -9999  29.0059   \n",
      "2  3600.0   0.2  0.0  0.05  39.8653   0.05  0.0  None       -9999  28.9876   \n",
      "3  3600.0   0.2  0.0  0.05  39.8562   0.05  0.0  None       -9999  28.9946   \n",
      "4  3600.0   0.2  0.0  0.05  40.0631   0.05  0.0  None       -9999  28.7625   \n",
      "\n",
      "       lon   alt  Kchan  ascendingI  Shape_Leng  Shape_Le_1  \\\n",
      "0 -96.0119  9.59      0     2034014    0.021074    0.021074   \n",
      "1 -96.0029  9.59      0     2034015    0.011898    0.011898   \n",
      "2 -95.9992  9.59      0     2071131    0.003975    0.003975   \n",
      "3 -96.0019  9.59      0     2034016    0.011638    0.011638   \n",
      "4 -96.0020  1.83      0     2071148    0.002016    0.002016   \n",
      "\n",
      "                                                                                                                                                                                                                                  geometry  \n",
      "0  LINESTRING (-96.02196 29.02037, -96.02186 29.02010, -96.02090 29.01974, -96.01945 29.01943, -96.01427 29.01804, -96.00931 29.01719, -96.00813 29.01609, -96.00779 29.01518, -96.00779 29.01330, -96.00751 29.01093, -96.00667 29.01034)  \n",
      "1                                                                                  LINESTRING (-96.00667 29.01034, -96.00620 29.01002, -96.00324 29.00704, -96.00262 29.00510, -96.00234 29.00321, -96.00186 29.00175, -96.00117 29.00026)  \n",
      "2                                          LINESTRING (-96.00024 28.98924, -95.99997 28.98902, -95.99972 28.98872, -95.99947 28.98846, -95.99923 28.98796, -95.99907 28.98730, -95.99898 28.98677, -95.99901 28.98596, -95.99909 28.98567)  \n",
      "3                                                              LINESTRING (-96.00117 29.00026, -96.00103 28.99919, -96.00097 28.99818, -96.00122 28.99716, -96.00191 28.99508, -96.00197 28.99248, -96.00134 28.99083, -96.00024 28.98924)  \n",
      "4                                                                                                                                                                  LINESTRING (-96.00230 28.76346, -96.00197 28.76247, -96.00154 28.76160)  \n",
      "MASK: None\n",
      "found 2014 segments\n",
      "found 2000 ref_keys\n",
      "found 15 headwater segments\n",
      "found 2 terminal segments\n",
      "of those, 0 had non-standard terminal keys\n",
      "identified at least 0 segments with circular references testing to the fourth level\n",
      "visited 2014 segments\n",
      "found 13 junctions in 13 junction nodes\n",
      "found 26 confluence segments\n",
      "CONUS_ge5\n",
      "reading -- dataset: /home/chp20/github/t-route/test/input/geo/Channels/RouteLink_CONUS.nwm.v3.0.11.nc; layer: 0; driver: NetCDF\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if ENV_IS_CL:\n",
    "    root = \"/content/t-route-jsh/\"\n",
    "elif not ENV_IS_CL:\n",
    "    root = os.path.dirname(os.path.abspath(\"\"))\n",
    "test_folder = os.path.join(root, r\"test\")\n",
    "geo_input_folder = os.path.join(test_folder, r\"input\", r\"geo\")\n",
    "\n",
    "supernetworks = {}\n",
    "\"\"\"##NHD Subset (Brazos/Lower Colorado)\"\"\"\n",
    "supernetworks.update({\"Brazos_LowerColorado_ge5\": {\"data\": None, \"values\": None}})\n",
    "\"\"\"##NHD CONUS order 5 and greater\"\"\"\n",
    "supernetworks.update({\"CONUS_ge5\": {\"data\": None, \"values\": None}})\n",
    "\"\"\"These are large -- be careful\"\"\"\n",
    "supernetworks.update({\"CONUS_FULL_RES_v20\": {\"data\": None, \"values\": None}})\n",
    "supernetworks.update(\n",
    "    {\"CONUS_Named_Streams\": {\"data\": None, \"values\": None}}\n",
    ")  # create a subset of the full resolution by reading the GNIS field\n",
    "\n",
    "debuglevel = -1\n",
    "verbose = False\n",
    "\n",
    "# The CONUS_ge5 and Brazos_LowerColorado_ge5 datasets are included\n",
    "# in the github test folder and are extracts from the NHD version 1.2 datasets\n",
    "# from https://www.nohrsc.noaa.gov/pub/staff/keicher/NWM_live/web/data_tools/\n",
    "#\n",
    "# The CONUS_FULL_RES file was generated from the RouteLink file in the parameter\n",
    "# archive and converted to a compressed NetCDF via the following command:\n",
    "# nccopy -d1 -s RouteLink_NWMv2.0_20190517_cheyenne_pull.nc RouteLink_NWMv2.0_20190517_cheyenne_pull.nc4s\n",
    "# TODO: Explain CONUS_Named_Streams\n",
    "# CONUS_Named_Streams was generated by intersecting the FULL_RES file ...\n",
    "# of the data in the nohrsc-hosted archive but are too large to efficiently\n",
    "# package inside of the repository.\n",
    "\n",
    "for sn in supernetworks:\n",
    "    print(sn)\n",
    "    supernetworks[sn][\"data\"], supernetworks[sn][\"values\"] = nnu.set_networks(\n",
    "        supernetwork=sn,\n",
    "        geo_input_folder=geo_input_folder,\n",
    "        verbose=verbose,\n",
    "        debuglevel=debuglevel,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_fuRiEXFsvOK"
   },
   "source": [
    "### With this, we can separate the different rivers in the network\n",
    "Once a 'connection' object has been created with a representation of the river network, we can traverse that object and perform calculations -- in the example below, we parallelize the process of traversing the independent portions of the network and then serially compute the number of junctions. This corresponds to the \"**System-level parallelization**\" mentioned as _item 1_ above.\n",
    "### NOW for the next step\n",
    "We could compute total upstream length or (and this is the real goal) flow due to incoming lateral contributions from the land accumulated over the entire upstream network. That second calculation can also be parallelized but we have to figure out how to accomplish intelligently so that the collective calculation is network-aware. Such a parallelization would be the \"**Network-level parallelization of interconnected reaches**\" mentioned as _item 2_ above. The upstream length will depend on the number of upstream branches and their configuration, so there has to be some concept of stream order and topology built into the parallelization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 25\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# terminal_keys = supernetworks['CONUS_ge5']['values'][4]\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# circular_keys = supernetworks['CONUS_ge5']['values'][6]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# terminal_keys_super = terminal_keys - circular_keys\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# con = supernetworks['CONUS_ge5']['values'][0]\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# terminal_code = supernetworks['CONUS_ge5']['data']['terminal_code']\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m\"\"\"Full Res NHD\"\"\"\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m terminal_keys \u001b[38;5;241m=\u001b[39m \u001b[43msupernetworks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCONUS_FULL_RES_v20\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     26\u001b[0m circular_keys \u001b[38;5;241m=\u001b[39m supernetworks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCONUS_FULL_RES_v20\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m6\u001b[39m]\n\u001b[1;32m     27\u001b[0m terminal_keys_super \u001b[38;5;241m=\u001b[39m terminal_keys \u001b[38;5;241m-\u001b[39m circular_keys\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# parallel compute\n",
    "import time\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "\"\"\"Test\"\"\"\n",
    "# terminal_keys = test_return_values[4]\n",
    "# circular_keys = test_return_values[6]\n",
    "# terminal_keys_super = terminal_keys - circular_keys\n",
    "# con = test_return_values[0]\n",
    "# terminal_code = test_terminal_code\n",
    "\"\"\"Streams of NHD order 5 or greater confluent to the Brazos and Lower Colorado\"\"\"\n",
    "# terminal_keys = supernetworks['Brazos_LowerColorado_ge5']['values'][4]\n",
    "# circular_keys = supernetworks['Brazos_LowerColorado_ge5']['values'][6]\n",
    "# terminal_keys_super = terminal_keys - circular_keys\n",
    "# con = supernetworks['Brazos_LowerColorado_ge5']['values'][0]\n",
    "# terminal_code = supernetworks['Brazos_LowerColorado_ge5']['data']['terminal_code']\n",
    "\"\"\"CONUS streams of NHD order 5 or greater\"\"\"\n",
    "# terminal_keys = supernetworks['CONUS_ge5']['values'][4]\n",
    "# circular_keys = supernetworks['CONUS_ge5']['values'][6]\n",
    "# terminal_keys_super = terminal_keys - circular_keys\n",
    "# con = supernetworks['CONUS_ge5']['values'][0]\n",
    "# terminal_code = supernetworks['CONUS_ge5']['data']['terminal_code']\n",
    "\"\"\"Full Res NHD\"\"\"\n",
    "terminal_keys = supernetworks[\"CONUS_FULL_RES_v20\"][\"values\"][4]\n",
    "circular_keys = supernetworks[\"CONUS_FULL_RES_v20\"][\"values\"][6]\n",
    "terminal_keys_super = terminal_keys - circular_keys\n",
    "con = supernetworks[\"CONUS_FULL_RES_v20\"][\"values\"][0]\n",
    "terminal_code = supernetworks[\"CONUS_FULL_RES_v20\"][\"data\"][\"terminal_code\"]\n",
    "\"\"\"Named Streams\"\"\"\n",
    "# terminal_keys = supernetworks['CONUS_Named_Streams']['values'][4]\n",
    "# circular_keys = supernetworks['CONUS_Named_Streams']['values'][6]\n",
    "# terminal_keys_super = terminal_keys - circular_keys\n",
    "# con = supernetworks['CONUS_Named_Streams']['values'][0]\n",
    "# terminal_code = supernetworks['CONUS_Named_Streams']['data']['terminal_code']\n",
    "\n",
    "\n",
    "def recursive_junction_read(\n",
    "    keys, network, terminal_code=0, verbose=False, debuglevel=0\n",
    "):\n",
    "    global con\n",
    "    for key in keys:\n",
    "        ckey = key\n",
    "        try:\n",
    "            ukeys = con[key][\"upstreams\"]\n",
    "            while not len(ukeys) >= 2 and not (ukeys == {terminal_code}):\n",
    "                if debuglevel <= -3:\n",
    "                    print(f\"segs at ckey {ckey}: {network['segment_count']}\")\n",
    "                # the terminal code will indicate a headwater\n",
    "                if debuglevel <= -4:\n",
    "                    print(ukeys)\n",
    "                (ckey,) = ukeys\n",
    "                ukeys = con[ckey][\"upstreams\"]\n",
    "            if ukeys == {terminal_code}:\n",
    "                if debuglevel <= -3:\n",
    "                    print(f\"headwater found at {ckey}\")\n",
    "                network[\"segment_count\"] += 1\n",
    "                if debuglevel <= -3:\n",
    "                    print(f\"segs at ckey {ckey}: {network['segment_count']}\")\n",
    "            elif len(ukeys) >= 2:\n",
    "                network[\"segment_count\"] += 1\n",
    "                if debuglevel <= -3:\n",
    "                    print(f\"junction found at {ckey} with upstreams {ukeys}\")\n",
    "                network[\"segment_count\"] += 1\n",
    "                if debuglevel <= -3:\n",
    "                    print(f\"segs at ckey {ckey}: {network['segment_count']}\")\n",
    "                network[\"junction_count\"] += 1  # the Terminal Segment\n",
    "                recursive_junction_read(\n",
    "                    ukeys,\n",
    "                    network,\n",
    "                    terminal_code=terminal_code,\n",
    "                    verbose=verbose,\n",
    "                    debuglevel=debuglevel,\n",
    "                )\n",
    "                # print(ukeys)\n",
    "                ukeys = con[ckey][\"upstreams\"]\n",
    "                ckey = ukeys\n",
    "        except:\n",
    "            if debuglevel <= -2:\n",
    "                print(f\"There is a problem with connection: {key}: {con[key]}\")\n",
    "\n",
    "\n",
    "def network_trace(nid, terminal_code=terminal_code, verbose=False, debuglevel=0):\n",
    "\n",
    "    network = {}\n",
    "    global con\n",
    "    us_length_total = 0\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\ntraversing upstream on network {nid}:\")\n",
    "    # try:\n",
    "    if 1 == 1:\n",
    "        network.update({\"junction_count\": 0})\n",
    "        network.update({\"segment_count\": 0})  # the Terminal Segment\n",
    "        recursive_junction_read(\n",
    "            [nid],\n",
    "            network,\n",
    "            verbose=verbose,\n",
    "            terminal_code=terminal_code,\n",
    "            debuglevel=debuglevel,\n",
    "        )\n",
    "        if verbose:\n",
    "            print(f\"junctions: {network['junction_count']}\")\n",
    "        if verbose:\n",
    "            print(f\"segments: {network['segment_count']}\")\n",
    "    # except Exception as exc:\n",
    "    #     print(exc)\n",
    "    # TODO: compute upstream length as a surrogate for the routing computation\n",
    "    return {nid: network, \"upstream_length\": us_length_total}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "Gz15rLB9WnBl",
    "outputId": "b872a326-258a-46f8-e9d6-66e0772a7957"
   },
   "outputs": [],
   "source": [
    "###continuing from previous cell\n",
    "networks = {\n",
    "    terminal_key: {}\n",
    "    for terminal_key in terminal_keys_super\n",
    "    # # Toggle these comments to try removing/isolating some of the larger networks\n",
    "    #                   if terminal_key in [\n",
    "    #                   if terminal_key not in [\n",
    "    #                           22811611 # Mississippi River, LA\n",
    "    #                           , 23832907 # Columbia River, WA\n",
    "    #                           , 626220 # Rio Grande, TX/MX\n",
    "    #                           , 21412883 # Colorado River, AZ/MX\n",
    "    #                           , 18524217 # Mobile River, AL\n",
    "    #                           , 15183793 # Atchafalaya, LA\n",
    "    #                   ]\n",
    "}\n",
    "debuglevel = -1\n",
    "verbose = False\n",
    "\n",
    "if verbose:\n",
    "    print(\"verbose output\")\n",
    "if verbose:\n",
    "    print(f\"number of Independent Networks to be analyzed is {len(super_networks)}\")\n",
    "if verbose:\n",
    "    print(f\"Multi-processing will use {multiprocessing.cpu_count()} CPUs\")\n",
    "if verbose:\n",
    "    print(f\"debuglevel is {debuglevel}\")\n",
    "\n",
    "start_time = time.time()\n",
    "results_serial = {}\n",
    "for nid, network in networks.items():\n",
    "    network.update(\n",
    "        network_trace(\n",
    "            nid, terminal_code=terminal_code, verbose=verbose, debuglevel=debuglevel\n",
    "        )[nid]\n",
    "    )\n",
    "print(\"--- %s seconds: serial compute ---\" % (time.time() - start_time))\n",
    "if debuglevel <= -1:\n",
    "    print(len(networks.items()))\n",
    "if debuglevel <= -2:\n",
    "    print(networks)\n",
    "\n",
    "## Notice that I'm not timing the initialization in each case,\n",
    "## which might be considered cheating a little bit.\n",
    "## I timed it for the first case for reference.\n",
    "nids = (nid for nid in networks)\n",
    "start_time = time.time()\n",
    "with multiprocessing.Pool() as pool:\n",
    "    print(\n",
    "        \"--- %s seconds: parallel overhead to load multiprocessing.Pool() ---\"\n",
    "        % (time.time() - start_time)\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    results = pool.map(network_trace, nids)\n",
    "    print(\n",
    "        \"--- %s seconds: parallel compute using default terminal_code ---\"\n",
    "        % (time.time() - start_time)\n",
    "    )\n",
    "if debuglevel <= -1:\n",
    "    print(len(results))\n",
    "if debuglevel <= -2:\n",
    "    print(results)\n",
    "\n",
    "nids = (nid for nid in networks)\n",
    "snt = partial(network_trace, terminal_code=terminal_code)\n",
    "with multiprocessing.Pool() as pool:\n",
    "    start_time = time.time()\n",
    "    results = pool.map(snt, nids)\n",
    "    print(\n",
    "        \"--- %s seconds: parallel compute using partial function ---\"\n",
    "        % (time.time() - start_time)\n",
    "    )\n",
    "if debuglevel <= -1:\n",
    "    print(len(results))\n",
    "if debuglevel <= -2:\n",
    "    print(results)\n",
    "\n",
    "nidsWtc = ([nid, terminal_code] for nid in networks)\n",
    "with multiprocessing.Pool() as pool:\n",
    "    start_time = time.time()\n",
    "    results = pool.starmap(network_trace, nidsWtc)\n",
    "    print(\n",
    "        \"--- %s seconds: parallel compute with list of lists and starmap ---\"\n",
    "        % (time.time() - start_time)\n",
    "    )\n",
    "if debuglevel <= -1:\n",
    "    print(len(results))\n",
    "if debuglevel <= -2:\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "text",
    "id": "pvfefJVLvxwe"
   },
   "source": [
    "### Colab output\n",
    "\n",
    "For the 14351 independent networks (and 2.7M segments) of the NWM Full Resolution dataset on a Google colaboratory VM, the algorithm steps through the segments of the different independent networks in order in about 4 seconds, regardless of the parallelization method. On a modest workstation at NWC with 12 cores, the compute time was approximately 2 seconds for a serial compute and 1.1 seconds for each of the parallel methods. Notably, because we are only breaking apart the independent networks, the maximum parallel speedup is limited by the size of the largest network -- the Mississippi River. By executing with the terminal node for the Mississippi River removed from the evaluation set, the serial calculation takes only 1.4 seconds and the parallel executions drop to approximately 0.4 seconds.<br>\n",
    "```\n",
    "--- 4.638037443161011 seconds: serial compute ---  \n",
    "14351  \n",
    "--- 0.0757453441619873 seconds: parallel overhead to load multiprocessing.Pool() ---  \n",
    "--- 4.344968557357788 seconds: parallel compute using default terminal_code ---  \n",
    "14351  \n",
    "--- 4.323424577713013 seconds: parallel compute using partial function ---  \n",
    "14351  \n",
    "--- 4.167566537857056 seconds: parallel compute with list of lists and starmap ---  \n",
    "14351  \n",
    "```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Network Analysis via Parallelization Demo",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
